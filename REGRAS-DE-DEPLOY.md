# ‚ö†Ô∏è REGRAS CR√çTICAS DE DEPLOY - LEIA ANTES DE QUALQUER DEPLOY!

## üö® REGRA #1: NUNCA RECRIAR CONTAINERS DE BANCO DE DADOS

**‚ùå NUNCA FA√áA:**
```bash
docker compose up -d --build  # RECRIA TODOS OS CONTAINERS = PERDE BANCO DE DADOS!
docker compose down && up -d  # REMOVE E RECRIA = PERDE BANCO DE DADOS!
docker compose build          # Rebuilda tudo, incluindo gera NOVAS SENHAS!
```

**‚úÖ SEMPRE FA√áA:**
```bash
# Para deploy de FRONTEND apenas:
cd /root/prevencao-radar-install/InstaladorVPS
docker compose -f docker-compose-producao.yml build --no-cache frontend
docker compose -f docker-compose-producao.yml up -d --no-deps frontend

# Para deploy de BACKEND apenas:
cd /root/prevencao-radar-install/InstaladorVPS
docker compose -f docker-compose-producao.yml build --no-cache backend
docker compose -f docker-compose-producao.yml up -d --no-deps backend

# Flags importantes:
# --no-deps = N√ÉO reinicia containers dependentes (PostgreSQL, MinIO)
# --no-cache = For√ßa rebuild sem usar cache (pega mudan√ßas novas)
```

---

## üîê REGRA #2: SENHAS DO BANCO S√ÉO GERADAS UMA VEZ E NUNCA MUDAM

**IMPORTANTE:** O `docker-compose-producao.yml` gera senhas aleat√≥rias na PRIMEIRA vez que os containers s√£o criados. Se voc√™ reconstruir as imagens, o docker-compose vai gerar NOVAS senhas, mas o banco postgres vai continuar com a senha ANTIGA!

**Resultado:** Backend n√£o consegue conectar no banco (erro: `password authentication failed`)

**SOLU√á√ÉO:**
- Use sempre `--no-deps` para n√£o recriar o container do postgres
- Se precisar reconstruir tudo do zero, use `docker compose down -v` (‚ö†Ô∏è PERDE TODOS OS DADOS!)

---

## üìã CHECKLIST OBRIGAT√ìRIO ANTES DE FAZER DEPLOY

### 1. Identificar o que mudou:
- [ ] Mudou c√≥digo do FRONTEND? (arquivos em `packages/frontend/src/`)
- [ ] Mudou c√≥digo do BACKEND? (arquivos em `packages/backend/src/`)
- [ ] Mudou BANCO DE DADOS? (migrations, schema)

### 2. Escolher comando correto:

#### ‚úÖ Se mudou APENAS FRONTEND:
```bash
ssh root@145.223.92.152
cd /root/prevencao-radar-install
git pull
cd InstaladorVPS
docker compose -f docker-compose-producao.yml build --no-cache frontend
docker compose -f docker-compose-producao.yml up -d --no-deps frontend
```

#### ‚úÖ Se mudou APENAS BACKEND:
```bash
ssh root@145.223.92.152
cd /root/prevencao-radar-install
git pull
cd InstaladorVPS
docker compose -f docker-compose-producao.yml build --no-cache backend
docker compose -f docker-compose-producao.yml up -d --no-deps backend
```

#### ‚úÖ Se mudou FRONTEND + BACKEND:
```bash
ssh root@145.223.92.152
cd /root/prevencao-radar-install
git pull
cd InstaladorVPS
docker compose -f docker-compose-producao.yml build --no-cache frontend backend
docker compose -f docker-compose-producao.yml up -d --no-deps frontend backend
```

#### ‚ö†Ô∏è Se mudou BANCO DE DADOS (migrations):
```bash
ssh root@145.223.92.152
cd /root/prevencao-radar-install
git pull
cd InstaladorVPS

# Apenas rebuild do backend (migrations rodam automaticamente no boot)
docker compose -f docker-compose-producao.yml build --no-cache backend
docker compose -f docker-compose-producao.yml up -d --no-deps backend

# Verificar logs para confirmar que migrations rodaram:
docker logs prevencao-backend-prod --tail 50
```

---

## üõë SE DEU ERRO: "password authentication failed for user postgres"

**Causa:** Voc√™ reconstruiu as imagens e o docker-compose gerou novas senhas diferentes das que o postgres est√° usando.

**Sintomas:**
- Backend n√£o conecta no banco
- Logs mostram: `password authentication failed for user "postgres"`
- Site fica em loop de loading

**Solu√ß√£o R√ÅPIDA (sem perder dados):**

```bash
# 1. Descobrir qual senha o backend est√° usando:
docker exec prevencao-backend-prod env | grep DB_PASSWORD

# 2. Descobrir qual senha o postgres est√° usando:
docker exec prevencao-postgres-prod env | grep POSTGRES_PASSWORD

# 3. Se forem diferentes, atualizar a senha do postgres:
docker exec prevencao-postgres-prod psql -U postgres -c "ALTER USER postgres WITH PASSWORD 'SENHA_DO_BACKEND_AQUI';"

# 4. Reiniciar o backend:
docker restart prevencao-backend-prod

# 5. Verificar se conectou:
docker logs prevencao-backend-prod --tail 30 | grep "Database connected"
```

---

## üõë SE VOC√ä RECRIOU O BANCO DE DADOS POR ENGANO

**Sintomas:**
- Tela de "First Setup" apareceu novamente
- Perdeu todas as configura√ß√µes/dados
- Volume do postgres foi deletado

**N√ÉO TEM MAIS VOLTA!** Os dados foram perdidos.

**Solu√ß√£o:**
```bash
# Parar tudo e come√ßar do zero
cd /root/prevencao-radar-install/InstaladorVPS
docker compose -f docker-compose-producao.yml down -v  # Remove volumes tamb√©m

# Rodar instalador novamente (cria banco do zero)
bash INSTALAR-AUTO.sh

# Avisar o usu√°rio que precisa:
# - Refazer First Setup
# - Reconfigurar APIs (Zanthus, WhatsApp, Evolution)
# - Reativar produtos
# - Refazer todas as configura√ß√µes
```

---

## üìù EXEMPLO REAL DE DEPLOY CORRETO

**Situa√ß√£o:** Implementei m√≥dulo de Produ√ß√£o com novas tabelas no banco

**Passos:**
```bash
# 1. Fazer commit local
git add -A
git commit -m "feat: Adiciona m√≥dulo de produ√ß√£o com dias por item"
git push

# 2. Deploy na VPS (APENAS BACKEND porque tem migration!)
ssh -i ~/.ssh/vps_prevencao root@145.223.92.152 "cd /root/prevencao-radar-install && git pull && cd InstaladorVPS && docker compose -f docker-compose-producao.yml build --no-cache backend && docker compose -f docker-compose-producao.yml up -d --no-deps backend"

# 3. Verificar se migrations rodaram e backend conectou
ssh -i ~/.ssh/vps_prevencao root@145.223.92.152 "docker logs prevencao-backend-prod --tail 50 | grep -E 'Database connected|migration ran|Server is running'"

# 4. Verificar se tabelas foram criadas
ssh -i ~/.ssh/vps_prevencao root@145.223.92.152 "docker exec prevencao-postgres-prod psql -U postgres -d prevencao_db -c \"SELECT table_name FROM information_schema.tables WHERE table_name LIKE '%production%';\""

# 5. Testar o site
curl http://145.223.92.152:3001/api/health
```

---

## üîç COMANDOS √öTEIS PARA VERIFICAR STATUS

```bash
# Ver containers rodando
docker ps

# Ver logs do backend
docker logs prevencao-backend-prod --tail 50 -f

# Ver logs do frontend
docker logs prevencao-frontend-prod --tail 50

# Verificar se banco est√° respondendo
docker exec prevencao-postgres-prod psql -U postgres -d prevencao_db -c 'SELECT COUNT(*) FROM users;'

# Verificar volumes (N√ÉO DEVEM SER DELETADOS!)
docker volume ls

# Verificar quantas tabelas existem no banco
docker exec prevencao-postgres-prod psql -U postgres -d prevencao_db -c '\dt' | wc -l

# Ver senha atual do postgres
docker exec prevencao-postgres-prod env | grep POSTGRES_PASSWORD

# Ver senha que o backend est√° usando
docker exec prevencao-backend-prod env | grep DB_PASSWORD
```

---

## üìÅ DIFEREN√áA ENTRE docker-compose.yml E docker-compose-producao.yml

### `docker-compose.yml` (Desenvolvimento Local)
- Senhas SIMPLES e fixas (postgres123, test-api-token)
- Sem SSL/TLS
- Sem volumes nomeados persistentes
- Portas diretas (3000, 3001, 5432)
- **NUNCA usar em produ√ß√£o!**

### `docker-compose-producao.yml` (VPS)
- Senhas FORTES geradas automaticamente na cria√ß√£o
- Configura√ß√µes de seguran√ßa
- Volumes nomeados persistentes (dados n√£o s√£o perdidos)
- Container de cron para tarefas agendadas
- Healthchecks configurados
- **SEMPRE usar em produ√ß√£o!**

**IMPORTANTE:** Se voc√™ reconstruir com `docker-compose-producao.yml`, ele vai gerar NOVAS senhas. Por isso sempre use `--no-deps` para n√£o recriar o postgres!

---

## ‚ùó MEMORIZAR ISSO:

1. **--no-deps** = N√ÉO mexe em PostgreSQL/MinIO (preserva senhas e dados)
2. **--no-cache** = Pega c√≥digo novo do Git
3. **Sempre especificar QUAL container atualizar** (frontend, backend, ou ambos)
4. **NUNCA usar `down`** a menos que queira come√ßar do zero
5. **docker-compose-producao.yml gera senhas NOVAS a cada build** - por isso use --no-deps!

---

## üìû SE TIVER D√öVIDA

**ANTES** de rodar qualquer comando de deploy:
1. Pare e pense: "Vou recriar o banco de dados com esse comando?"
2. Pare e pense: "Vou gerar novas senhas diferentes?"
3. Se a resposta for "SIM" ou "N√ÉO SEI", **N√ÉO RODE O COMANDO!**
4. Consulte este documento novamente
5. Use `--no-deps` para garantir

---

## üéì LI√á√ïES APRENDIDAS (09/01/2026)

### Problema que aconteceu:
1. Reconstru√≠ imagens com `docker-compose-producao.yml`
2. Docker gerou NOVAS senhas aleat√≥rias
3. Backend tentou usar senha NOVA
4. Postgres tinha senha ANTIGA
5. Backend n√£o conseguiu conectar

### Solu√ß√£o aplicada:
1. Mantive postgres com senha antiga (preservou dados)
2. Descobri qual senha o backend estava usando
3. Alterei a senha do postgres para a senha do backend novo
4. Backend conectou e migrations rodaram
5. Nenhum dado foi perdido!

### Aprendizado:
- Senhas s√£o geradas na CRIA√á√ÉO dos containers
- Se rebuildar, gera novas senhas
- Sempre usar `--no-deps` para n√£o recriar postgres
- Se errar, d√° pra corrigir alterando senha do postgres

---

## üÜò SE DEU ERRO: Hash de senha corrompido/desatualizado

**Problema encontrado em 11/01/2026:**

**Sintomas:**
- Backend n√£o conecta no banco mesmo com senhas corretas no .env
- Erro: `password authentication failed for user "postgres"`
- Senhas conferem mas autentica√ß√£o falha
- `database.connected = false` na API health

**Causa:**
- Hash da senha no PostgreSQL ficou desatualizado/corrompido
- Ocorre ap√≥s m√∫ltiplos rebuilds ou recria√ß√µes de containers
- Mesmo com senha correta no .env, o hash interno n√£o corresponde

**Solu√ß√£o R√ÅPIDA:**

```bash
# 1. Verificar se as senhas est√£o iguais:
docker exec prevencao-backend-prod env | grep DB_PASSWORD
docker exec prevencao-postgres-prod env | grep POSTGRES_PASSWORD

# 2. Se estiverem iguais mas ainda d√° erro, resetar hash da senha:
SENHA=$(docker exec prevencao-postgres-prod env | grep POSTGRES_PASSWORD | cut -d'=' -f2)
docker exec prevencao-postgres-prod psql -U postgres -c "ALTER USER postgres WITH PASSWORD '$SENHA';"

# 3. Reiniciar backend:
docker restart prevencao-backend-prod

# 4. Verificar se conectou:
curl http://localhost:3001/api/health | grep "connected"
# Deve retornar: "connected":true
```

**Importante:**
- Este comando N√ÉO altera a senha, apenas atualiza o hash interno do PostgreSQL
- √â seguro executar mesmo em produ√ß√£o
- N√£o afeta dados ou conex√µes existentes

---

## üßπ LIMPEZA DE RECURSOS OBSOLETOS

**Quando fazer:** Ap√≥s m√∫ltiplos deploys e testes, containers/imagens/volumes n√£o usados se acumulam.

**Como identificar:**

```bash
# Ver containers inativos
docker ps -a --filter 'status=created' --filter 'status=exited'

# Ver volumes n√£o linkados
docker volume ls

# Ver tamanho de volumes
docker system df -v

# Ver imagens n√£o usadas
docker images

# Ver build cache (pode acumular 30GB+!)
docker system df
```

**Recursos seguros para remover:**

1. **Containers com status "Created"** - nunca rodaram
2. **Imagens sem TAG "latest"** ou duplicadas
3. **Volumes com LINKS=0** (n√£o linkados a nenhum container)
4. **Build cache antigo** (libera muito espa√ßo!)

**NUNCA remova:**
- Volumes linkados (LINKS > 0)
- Containers com `-prod` no nome
- Volumes de produ√ß√£o

**Comandos de limpeza:**

```bash
# Remover apenas recursos n√£o usados (SEGURO)
docker system prune -a

# Limpar build cache (libera MUITO espa√ßo)
docker builder prune --all --force

# Remover volume espec√≠fico (CUIDADO!)
docker volume rm nome-do-volume  # S√≥ se LINKS=0
```

---

---

## üñ•Ô∏è VPS 46 - M√öLTIPLOS CLIENTES (ATEN√á√ÉO ESPECIAL!)

### ‚ö†Ô∏è ESTRUTURA DIFERENTE DAS OUTRAS VPS

A VPS 46 (`46.202.150.64`) tem uma estrutura **multi-tenant** com v√°rios clientes instalados. **N√ÉO** √© igual √†s outras VPS!

### üìç IPs e Identifica√ß√£o das VPS

| VPS | IP | Uso | Diret√≥rio Principal |
|-----|-----|-----|---------------------|
| VPS 46 | `46.202.150.64` | **PRODU√á√ÉO** (Multi-clientes) | `/root/clientes/[cliente]` |
| VPS 31 | `31.97.82.235` | Outras finalidades | `/root/NOVO-PREVEN-O` |

> ‚ö†Ô∏è **VPS 145 (145.223.92.152) foi descontinuada - n√£o usar mais!**

### üè¢ Clientes na VPS 46

```
/root/clientes/
‚îú‚îÄ‚îÄ tradicao/          # Cliente Tradi√ß√£o SJC
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îî‚îÄ‚îÄ CREDENCIAIS.txt
‚îú‚îÄ‚îÄ piratininga/       # Cliente Piratininga
‚îî‚îÄ‚îÄ central/           # Cliente Central
```

### üì¶ Containers por Cliente na VPS 46

| Cliente | Frontend | Backend | Postgres | MinIO |
|---------|----------|---------|----------|-------|
| tradicao | `prevencao-tradicao-frontend` | `prevencao-tradicao-backend` | `prevencao-tradicao-postgres` | `prevencao-tradicao-minio` |
| piratininga | `prevencao-piratininga-frontend` | `prevencao-piratininga-backend` | `prevencao-piratininga-postgres` | `prevencao-piratininga-minio` |
| central | `prevencao-central-frontend` | `prevencao-central-backend` | `prevencao-central-postgres` | `prevencao-central-minio` |

‚ö†Ô∏è **ATEN√á√ÉO:** Tamb√©m existem containers `prevencao-frontend-prod` e `prevencao-backend-prod` na VPS 46, mas **N√ÉO s√£o usados pelos clientes**! S√£o de uma instala√ß√£o antiga/teste.

### üìÇ Estrutura de C√≥digo na VPS 46

```
/root/
‚îú‚îÄ‚îÄ prevencao-radar-repo/      # ‚Üê C√ìDIGO FONTE (git clone do TESTES-)
‚îÇ   ‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frontend/          # C√≥digo do frontend
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backend/           # C√≥digo do backend
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ clientes/
‚îÇ   ‚îî‚îÄ‚îÄ tradicao/
‚îÇ       ‚îú‚îÄ‚îÄ docker-compose.yml # ‚Üê Referencia o c√≥digo de /root/prevencao-radar-repo
‚îÇ       ‚îî‚îÄ‚îÄ .env               # ‚Üê Configura√ß√µes espec√≠ficas do cliente
‚îî‚îÄ‚îÄ prevencao-radar-install/   # ‚ö†Ô∏è N√ÉO USAR - instala√ß√£o antiga
```

### ‚úÖ DEPLOY CORRETO NA VPS 46 (Cliente Tradi√ß√£o)

```bash
# 1. Conectar na VPS 46
ssh root@46.202.150.64

# 2. Atualizar c√≥digo fonte
cd /root/prevencao-radar-repo
git pull origin TESTE

# 3. Ir para pasta do cliente
cd /root/clientes/tradicao

# 4. Build do frontend (se mudou frontend)
docker compose build --no-cache frontend
docker compose up -d --no-deps frontend

# 5. Build do backend (se mudou backend ou migrations)
docker compose build --no-cache backend
docker compose up -d --no-deps backend

# 6. Verificar logs
docker logs prevencao-tradicao-backend --tail 50
docker logs prevencao-tradicao-frontend --tail 20
```

### üîÑ Comando √önico para Deploy Completo (Tradi√ß√£o)

```bash
# Frontend + Backend
ssh root@46.202.150.64 "cd /root/prevencao-radar-repo && git pull origin TESTE && cd /root/clientes/tradicao && docker compose build --no-cache frontend backend && docker compose up -d --no-deps frontend backend"

# Apenas Frontend
ssh root@46.202.150.64 "cd /root/prevencao-radar-repo && git pull origin TESTE && cd /root/clientes/tradicao && docker compose build --no-cache frontend && docker compose up -d --no-deps frontend"

# Apenas Backend
ssh root@46.202.150.64 "cd /root/prevencao-radar-repo && git pull origin TESTE && cd /root/clientes/tradicao && docker compose build --no-cache backend && docker compose up -d --no-deps backend"
```

### üîç Verificar Status dos Clientes

```bash
# Ver todos os containers da VPS 46
ssh root@46.202.150.64 "docker ps --format 'table {{.Names}}\t{{.Status}}'"

# Ver logs do tradicao
ssh root@46.202.150.64 "docker logs prevencao-tradicao-backend --tail 30"
ssh root@46.202.150.64 "docker logs prevencao-tradicao-frontend --tail 10"

# Verificar banco do tradicao
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c '\\dt'"
```

### ‚ùå ERROS COMUNS NA VPS 46

1. **Atualizou container errado**: Verificar se est√° usando `prevencao-tradicao-*` e n√£o `prevencao-*-prod`
2. **Git pull no diret√≥rio errado**: Deve ser em `/root/prevencao-radar-repo`, n√£o em `/root/prevencao-radar-install`
3. **Docker compose no lugar errado**: Deve rodar em `/root/clientes/tradicao`, n√£o em `/root/prevencao-radar-repo`

### üéì Li√ß√£o Aprendida (20/01/2026)

**Problema:** Deploy n√£o funcionava na VPS 46 - site continuava mostrando vers√£o antiga.

**Causa:** Estava atualizando `/root/prevencao-radar-install` e o container `prevencao-frontend-prod`, que n√£o tem rela√ß√£o com o site `tradicao.prevencaonoradar.com.br`.

**Solu√ß√£o:**
1. Identificar que VPS 46 tem m√∫ltiplos clientes
2. Descobrir que o c√≥digo est√° em `/root/prevencao-radar-repo`
3. Descobrir que o docker-compose est√° em `/root/clientes/tradicao`
4. Usar os containers corretos: `prevencao-tradicao-frontend` e `prevencao-tradicao-backend`

---

## üîó CONEX√ÉO COM INTERSOLID - LOCAL vs VPS (ATEN√á√ÉO!)

### ‚ö†Ô∏è PROBLEMA: C√≥digo local funciona mas VPS n√£o conecta no ERP

**Contexto:**
- **Local (Windows)**: Conecta diretamente no IP da m√°quina Intersolid (ex: `10.6.1.102:3003`)
- **VPS (Docker)**: Conecta via t√∫nel SSH reverso que exp√µe a porta no host

**O problema:**
- O container Docker na VPS est√° numa rede isolada
- `127.0.0.1` dentro do container aponta para o pr√≥prio container, N√ÉO para o host
- O t√∫nel SSH escuta no host da VPS, n√£o dentro do container

### ‚úÖ SOLU√á√ÉO CORRETA: Usar configura√ß√£o do banco de dados

**NUNCA fa√ßa isso no c√≥digo:**
```typescript
// ‚ùå ERRADO - n√£o funciona no container Docker
if (isProduction) {
  erpApiUrl = `http://127.0.0.1:3003/v1/produtos`;
}
```

**SEMPRE fa√ßa assim:**
```typescript
// ‚úÖ CORRETO - usa configura√ß√£o do banco que j√° tem o IP certo
if (process.env.ERP_PRODUCTS_API_URL) {
  // Desenvolvimento local: usa .env
  erpApiUrl = process.env.ERP_PRODUCTS_API_URL;
} else {
  // Produ√ß√£o (VPS): busca do banco de dados
  const apiUrl = await ConfigurationService.get('intersolid_api_url', null);
  const port = await ConfigurationService.get('intersolid_port', null);
  const endpoint = await ConfigurationService.get('intersolid_products_endpoint', '/v1/produtos');
  const baseUrl = port ? `${apiUrl}:${port}` : apiUrl;
  erpApiUrl = baseUrl ? `${baseUrl}${endpoint}` : 'http://mock-erp-api.com';
}
```

### üìù Configura√ß√£o do banco na VPS (j√° configurado)

| Chave | Valor | Descri√ß√£o |
|-------|-------|-----------|
| `intersolid_api_url` | `http://172.20.0.1` | Gateway Docker (acessa o host) |
| `intersolid_port` | `3003` | Porta do t√∫nel SSH |
| `intersolid_products_endpoint` | `/v1/produtos` | Endpoint de produtos |

**Por que `172.20.0.1`?**
- √â o gateway da rede Docker
- Permite o container acessar servi√ßos rodando no host da VPS
- O t√∫nel SSH reverso exp√µe a porta 3003 no host

### üéì Li√ß√£o Aprendida (21/01/2026)

**Problema:** Tela de Auditoria de Produ√ß√£o dava erro 500 - `ECONNREFUSED 127.0.0.1:3003`

**Causa:** C√≥digo usava IP fixo `127.0.0.1:3003` que n√£o funciona dentro do container Docker

**Solu√ß√£o:** Alterado para usar `ConfigurationService.get('intersolid_api_url')` igual √†s outras telas

**Arquivos que devem seguir esse padr√£o:**
- `products.controller.ts` ‚úÖ
- `production-audit.controller.ts` ‚úÖ (corrigido)
- `bip-webhook.service.ts` ‚úÖ
- `sales.service.ts` ‚úÖ
- Qualquer novo arquivo que conecte no Intersolid

---

## üî∂ REGRA #3: HOST ORACLE NA VPS √â DIFERENTE DO LOCAL!

### ‚ö†Ô∏è PROBLEMA COMUM AP√ìS DEPLOY

Ap√≥s fazer deploy, o Oracle para de conectar na VPS com erro:
```
‚ùå ORA-12170: Cannot connect. TCP connect timeout for host 10.6.1.100 port 1521
```

### üìç Causa

A configura√ß√£o de conex√£o Oracle √© salva na tabela `database_connections` do PostgreSQL.

| Ambiente | Host Correto | Por qu√™ |
|----------|--------------|---------|
| **Local** (desenvolvimento) | `10.6.1.100` | Conecta direto na rede local |
| **VPS** (produ√ß√£o) | `172.20.0.1` | Conecta via t√∫nel SSH pelo gateway Docker |

Quando voc√™ configura a conex√£o Intersolid **localmente**, o sistema salva `10.6.1.100`. Se essa configura√ß√£o for replicada para a VPS, ela n√£o funciona porque `10.6.1.100` n√£o existe na rede Docker da VPS.

### ‚úÖ Solu√ß√£o: Verificar e corrigir ap√≥s deploy

```bash
# 1. Verificar host atual
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"SELECT name, host, port FROM database_connections WHERE type = 'oracle';\""

# 2. Se estiver 10.6.1.100, corrigir para 172.20.0.1
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"UPDATE database_connections SET host = '172.20.0.1' WHERE name = 'Intersolid';\""

# 3. Reiniciar backend para recarregar configura√ß√£o
ssh root@46.202.150.64 "docker restart prevencao-tradicao-backend"

# 4. Verificar se conectou
ssh root@46.202.150.64 "docker logs prevencao-tradicao-backend --tail 20 | grep -i oracle"
```

**Logs esperados (sucesso):**
```
üì¶ Oracle config loaded from database_connections: Intersolid (172.20.0.1:1521/orcl.intersoul)
‚úÖ Oracle connection pool initialized
```

### üìã Checklist p√≥s-deploy (quando envolve Oracle)

- [ ] Verificar se host Oracle est√° `172.20.0.1` (n√£o `10.6.1.100`)
- [ ] Verificar se t√∫nel SSH est√° ativo: `ss -tlnp | grep 1521`
- [ ] Verificar logs do backend: `docker logs ... | grep oracle`

### üéì Li√ß√£o Aprendida (01/02/2026)

**Problema:** Ap√≥s deploy, Oracle parou de conectar na VPS.

**Causa:** A tabela `database_connections` tinha `host = '10.6.1.100'` (IP da rede local) em vez de `host = '172.20.0.1'` (gateway Docker que acessa o t√∫nel SSH).

**Solu√ß√£o:** Atualizar o host no banco PostgreSQL da VPS para `172.20.0.1` e reiniciar o backend.

**Preven√ß√£o futura:** Sempre verificar o host Oracle ap√≥s deploy ou ao configurar conex√£o Intersolid.

### üõ°Ô∏è SOLU√á√ÉO PERMANENTE: Vari√°veis de Ambiente

O `OracleService` j√° suporta uma **ordem de prioridade** para configura√ß√£o:

1. **Vari√°veis de ambiente** `ORACLE_CONNECT_STRING` (mais alta prioridade)
2. **Tabela `database_connections`** (PostgreSQL)
3. **Valores padr√£o hardcoded** (fallback)

**A solu√ß√£o para n√£o conflitar nunca mais:**

| Ambiente | Configura√ß√£o | Como aplicar |
|----------|--------------|--------------|
| **VPS (Docker)** | Vari√°vel de ambiente no docker-compose | `ORACLE_CONNECT_STRING=172.20.0.1:1521/orcl.intersoul` |
| **Local** | Usa tabela `database_connections` | Configurar host `10.6.1.100` na tela de Configura√ß√µes |

**Como configurar na VPS (docker-compose.yml do cliente):**

```yaml
services:
  backend:
    environment:
      # For√ßa o Oracle a usar o gateway Docker (t√∫nel SSH)
      ORACLE_CONNECT_STRING: "172.20.0.1:1521/orcl.intersoul"
      ORACLE_USER: "POWERBI"
      ORACLE_PASSWORD: "OdRz6J4LY6Y6"
```

**Benef√≠cios:**
- VPS **sempre** usa `172.20.0.1` via vari√°vel de ambiente
- Local **sempre** usa o que est√° configurado na tela (tabela do banco)
- **Nunca mais conflita!** Cada ambiente tem sua config isolada

---

---

## üßπ REGRA #4: SEMPRE LIMPAR CACHE DO DOCKER AP√ìS DEPLOY!

### ‚ö†Ô∏è PROBLEMA: Disco enche ap√≥s m√∫ltiplos deploys

O Docker acumula **cache de build** a cada execu√ß√£o de `docker compose build --no-cache`. Isso pode facilmente ocupar **30GB+ de espa√ßo** ap√≥s alguns deploys, causando:

- VPS travando ou ficando sem resposta
- Builds falhando por falta de espa√ßo
- Erro "No space left on device"

### üìä Exemplo Real (02/02/2026)

```
ANTES do deploy:  49GB/50GB usado (1GB livre)
DEPOIS do deploy: VPS travou - disco 100% cheio
AP√ìS limpeza:     34GB/96GB usado (62GB livre)
```

### ‚úÖ PROCESSO CORRETO DE DEPLOY (COM LIMPEZA)

```bash
# 1. Atualizar c√≥digo
cd /root/prevencao-radar-repo && git pull origin TESTE

# 2. Ir para o cliente
cd /root/clientes/tradicao

# 3. Build com --no-cache (necess√°rio para pegar mudan√ßas no c√≥digo)
docker compose build --no-cache frontend backend

# 4. Subir containers (--no-deps preserva PostgreSQL/MinIO)
docker compose up -d --no-deps frontend backend

# 5. ‚ö†Ô∏è IMPORTANTE: Limpar cache do Docker ap√≥s o build
docker builder prune -f
docker image prune -f
```

### üìã Tabela de Refer√™ncia R√°pida

| Flag/Comando | O que faz |
|--------------|-----------|
| `--no-cache` | For√ßa rebuild completo (pega altera√ß√µes no c√≥digo) |
| `--no-deps` | N√£o recria PostgreSQL/MinIO (preserva dados e senhas) |
| `docker builder prune -f` | Limpa cache de build (libera muito espa√ßo) |
| `docker image prune -f` | Remove imagens antigas n√£o usadas |

### üîÑ Script de Deploy Completo (Recomendado)

Crie o arquivo `/root/deploy-cliente.sh`:

```bash
#!/bin/bash
# Script de deploy seguro com limpeza de cache

CLIENTE=${1:-tradicao}

echo "üöÄ Iniciando deploy para cliente: $CLIENTE"

# Atualizar c√≥digo
cd /root/prevencao-radar-repo && git pull origin TESTE

# Build e deploy
cd /root/clientes/$CLIENTE
docker compose build --no-cache frontend backend
docker compose up -d --no-deps frontend backend

# Limpar cache (IMPORTANTE!)
echo "üßπ Limpando cache do Docker..."
docker builder prune -f
docker image prune -f

# Verificar
echo "‚úÖ Deploy conclu√≠do! Verificando containers..."
docker compose ps

echo "üìä Espa√ßo em disco:"
df -h /

echo "üéâ Pronto!"
```

**Uso:**
```bash
chmod +x /root/deploy-cliente.sh
./deploy-cliente.sh tradicao    # Deploy no cliente Tradi√ß√£o
./deploy-cliente.sh piratininga # Deploy no cliente Piratininga
```

### üîç Como verificar espa√ßo do Docker

```bash
# Ver uso geral do Docker
docker system df

# Ver detalhado (imagens, containers, volumes, cache)
docker system df -v

# Ver espa√ßo em disco da VPS
df -h
```

### üéì Li√ß√£o Aprendida (02/02/2026)

**Problema:** VPS 46 travou durante deploy - disco encheu e SSH parou de responder.

**Causa:** O `docker compose build --no-cache` acumula cache a cada execu√ß√£o. Sem limpeza peri√≥dica, o disco encheu rapidamente (49GB ‚Üí 100%).

**Solu√ß√£o:**
1. Usu√°rio aumentou limite da VPS de 50GB para 100GB
2. Ap√≥s deploy, executar `docker builder prune -f && docker image prune -f`
3. Isso liberou ~30GB de espa√ßo

**Preven√ß√£o:**
- Sempre limpar cache ap√≥s o deploy
- Verificar `df -h` antes de fazer deploy
- Se espa√ßo < 10GB, limpar antes do deploy

---

## üè™ REGRA #5: VERIFICAR MULTI-LOJA ANTES DO DEPLOY!

### ‚ö†Ô∏è PROBLEMA: Deploy funciona mas sistema n√£o separa dados por loja

Quando o suporte multi-loja n√£o est√° configurado corretamente, o sistema pode:
- Misturar dados entre lojas diferentes
- N√£o mostrar filtro de loja corretamente
- Perder bipagens/vendas por falta de associa√ß√£o com cod_loja

### üìã CHECKLIST MULTI-LOJA (ANTES DO DEPLOY)

#### 1. Verificar colunas cod_loja no PostgreSQL

```bash
# Conectar no banco do cliente
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
SELECT table_name, column_name
FROM information_schema.columns
WHERE column_name = 'cod_loja'
ORDER BY table_name;
\""
```

**Tabelas que DEVEM ter cod_loja:**
| Tabela | Obrigat√≥rio |
|--------|-------------|
| `bips` | ‚úÖ Sim |
| `sells` | ‚úÖ Sim |
| `sectors` | ‚úÖ Sim |
| `hort_frut_boxes` | ‚úÖ Sim |
| `products` | ‚ö†Ô∏è Opcional (filtro por loja) |

#### 2. Se faltar coluna cod_loja, adicionar:

```bash
# Adicionar cod_loja em bips (se n√£o existir)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
ALTER TABLE bips ADD COLUMN IF NOT EXISTS cod_loja INTEGER;
\""

# Adicionar cod_loja em sells (se n√£o existir)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
ALTER TABLE sells ADD COLUMN IF NOT EXISTS cod_loja INTEGER;
\""

# Adicionar cod_loja em sectors (se n√£o existir)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
ALTER TABLE sectors ADD COLUMN IF NOT EXISTS cod_loja INTEGER;
\""

# Adicionar cod_loja em hort_frut_boxes (se n√£o existir)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
ALTER TABLE hort_frut_boxes ADD COLUMN IF NOT EXISTS cod_loja INTEGER;
\""
```

#### 3. Verificar configura√ß√£o de lojas no sistema

```bash
# Ver lojas cadastradas (companies)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
SELECT id, name, cod_loja, apelido FROM companies ORDER BY cod_loja;
\""
```

**Esperado:** Cada loja deve ter um `cod_loja` √∫nico e um `apelido` para identifica√ß√£o

#### 4. Verificar se dados antigos t√™m cod_loja

```bash
# Verificar bipagens sem cod_loja
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
SELECT COUNT(*) as sem_loja FROM bips WHERE cod_loja IS NULL;
\""

# Verificar vendas sem cod_loja
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
SELECT COUNT(*) as sem_loja FROM sells WHERE cod_loja IS NULL;
\""
```

**Se houver registros sem cod_loja**, atualizar com a loja padr√£o:

```bash
# Atualizar bipagens antigas para loja padr√£o (ex: cod_loja = 1)
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
UPDATE bips SET cod_loja = 1 WHERE cod_loja IS NULL;
\""

# Atualizar vendas antigas para loja padr√£o
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
UPDATE sells SET cod_loja = 1 WHERE cod_loja IS NULL;
\""
```

### ‚úÖ PROCESSO COMPLETO DE DEPLOY COM VERIFICA√á√ÉO MULTI-LOJA

```bash
# 1. Conectar na VPS
ssh root@46.202.150.64

# 2. VERIFICAR MULTI-LOJA PRIMEIRO
docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c "
SELECT table_name FROM information_schema.columns
WHERE column_name = 'cod_loja' AND table_name IN ('bips', 'sells', 'sectors', 'hort_frut_boxes')
ORDER BY table_name;
"

# 3. Se OK, prosseguir com deploy
cd /root/prevencao-radar-repo && git pull origin TESTE

# 4. Build e deploy
cd /root/clientes/tradicao
docker compose build --no-cache frontend backend
docker compose up -d --no-deps frontend backend

# 5. Limpar cache
docker builder prune -f && docker image prune -f

# 6. Verificar logs
docker logs prevencao-tradicao-backend --tail 30
```

### üéì Li√ß√£o Aprendida

**Problema:** Sistema deployado mas dados apareciam misturados entre lojas.

**Causa:** Tabelas n√£o tinham coluna `cod_loja` ou dados antigos estavam sem associa√ß√£o de loja.

**Solu√ß√£o:** Sempre verificar estrutura multi-loja ANTES do deploy e corrigir se necess√°rio.

**Preven√ß√£o:**
- Executar checklist multi-loja antes de cada deploy
- Verificar se migrations de multi-loja rodaram corretamente
- Confirmar que dados antigos foram migrados com cod_loja

---

## üóÇÔ∏è REGRA #6: VERIFICAR MAPEAMENTO DIN√ÇMICO (N√ÉO HARDCODE!)

### ‚ö†Ô∏è PROBLEMA: C√≥digo hardcoded impede uso com outros ERPs

Se o c√≥digo usa tabelas/schema hardcoded como `INTERSOLID.TAB_PRODUTO`, o sistema s√≥ funciona com esse ERP espec√≠fico. Para suportar m√∫ltiplos ERPs, **TODO c√≥digo deve usar o MappingService**.

### üìã CHECKLIST ANTES DO DEPLOY (C√≥digo Din√¢mico)

#### 1. Verificar se h√° refer√™ncias hardcoded no c√≥digo novo

```bash
# Buscar por INTERSOLID hardcoded no c√≥digo
grep -r "INTERSOLID\." packages/backend/src --include="*.ts"

# Se encontrar algo, o c√≥digo precisa ser migrado para MappingService!
```

**Resultado esperado:** `0 matches` (nenhum hardcode)

#### 2. Padr√£o CORRETO (usar MappingService)

```typescript
// ‚ùå ERRADO - Hardcoded (n√£o faz deploy assim!)
const sql = `SELECT * FROM INTERSOLID.TAB_PRODUTO WHERE ...`;

// ‚úÖ CORRETO - Din√¢mico via MappingService
import { MappingService } from '../services/mapping.service';

const schema = await MappingService.getSchema();
const tabProduto = `${schema}.${await MappingService.getRealTableName('TAB_PRODUTO', 'TAB_PRODUTO')}`;
const sql = `SELECT * FROM ${tabProduto} WHERE ...`;
```

#### 3. Se adicionar nova funcionalidade que usa tabelas Oracle:

**ANTES de fazer commit/deploy:**

1. Verificar se usa `MappingService.getSchema()` para o schema
2. Verificar se usa `MappingService.getRealTableName()` para as tabelas
3. O segundo par√¢metro √© o fallback (valor padr√£o se n√£o houver mapeamento)

### üìä Tabelas dispon√≠veis no MappingService

| ID da Tabela | Descri√ß√£o | Usado em |
|--------------|-----------|----------|
| `TAB_PRODUTO` | Produtos | Todos os m√≥dulos |
| `TAB_PRODUTO_LOJA` | Pre√ßos por loja | Bipagens, Produtos |
| `TAB_PRODUTO_PDV` | Vendas PDV | Frente de Caixa |
| `TAB_OPERADORES` | Operadores | Frente de Caixa, PDV |
| `TAB_FORNECEDOR` | Fornecedores | Compra/Venda, Pedidos |
| `TAB_PEDIDO` | Pedidos | Pedidos de Compra |
| `TAB_PEDIDO_PRODUTO` | Itens do Pedido | Ruptura Ind√∫stria |
| `TAB_NF` | Notas Fiscais | Compra/Venda |
| `TAB_NF_ITEM` | Itens da NF | Compra/Venda |

---

## üîÑ REGRA #7: ATUALIZAR TEMPLATE ERP AO ADICIONAR NOVAS TABELAS

### ‚ö†Ô∏è PROBLEMA: Nova funcionalidade n√£o funciona porque template ERP n√£o tem as tabelas

Quando voc√™ adiciona c√≥digo que usa uma nova tabela Oracle (ex: `TAB_NF_ITEM`), o template do ERP no banco de dados tamb√©m precisa ser atualizado, sen√£o o MappingService n√£o encontra o mapeamento.

### üìã CHECKLIST AO ADICIONAR NOVA TABELA/COLUNA

#### 1. Verificar se a tabela j√° existe no template

```bash
# Conectar no banco e ver o template atual
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
SELECT name,
       jsonb_pretty(mappings::jsonb->'tabelas') as tabelas
FROM erp_templates
WHERE name ILIKE '%intersolid%' AND is_active = true;
\""
```

#### 2. Se a tabela N√ÉO existe no template, adicionar:

**Op√ß√£o A: Via script (recomendado)**

```bash
# Usar o script update-template.js
cd /root/prevencao-radar-repo/packages/backend
node update-template.js producao
```

**Op√ß√£o B: Manualmente no banco**

```bash
# Exemplo: Adicionar TAB_NF_ITEM ao template
ssh root@46.202.150.64 "docker exec prevencao-tradicao-postgres psql -U postgres -d postgres_tradicao -c \"
UPDATE erp_templates
SET mappings = jsonb_set(
  mappings::jsonb,
  '{tabelas,TAB_NF_ITEM}',
  '{\"nome_real\": \"TAB_NF_ITEM\", \"colunas\": {\"numero_nf\": \"NUM_NF\", \"serie_nf\": \"NUM_SERIE_NF\", \"codigo_item\": \"COD_ITEM\"}}'::jsonb
)::text
WHERE name ILIKE '%intersolid%';
\""
```

#### 3. Atualizar tamb√©m o frontend (ConfiguracoesTabelas.jsx)

Se adicionou uma nova tabela, ela deve aparecer na tela de configura√ß√£o:

**Arquivo:** `packages/frontend/src/pages/ConfiguracoesTabelas.jsx`

1. Adicionar a tabela no `TABLE_CATALOG`
2. Adicionar os campos da tabela
3. Atualizar o subm√≥dulo correspondente em `BUSINESS_MODULES`

### üìù Exemplo Completo: Adicionando TAB_NOVA_TABELA

**Passo 1: C√≥digo backend (usar MappingService)**
```typescript
const schema = await MappingService.getSchema();
const tabNova = `${schema}.${await MappingService.getRealTableName('TAB_NOVA_TABELA', 'TAB_NOVA_TABELA')}`;
```

**Passo 2: Frontend (ConfiguracoesTabelas.jsx)**
```javascript
// Em TABLE_CATALOG, adicionar:
TAB_NOVA_TABELA: {
  name: 'Nova Tabela',
  description: 'Descri√ß√£o da tabela',
  fields: [
    { id: 'campo1', name: 'Campo 1', defaultColumn: 'COL_CAMPO1' },
    { id: 'campo2', name: 'Campo 2', defaultColumn: 'COL_CAMPO2' },
  ]
},

// Em BUSINESS_MODULES, adicionar ao subm√≥dulo:
{ id: 'meu_submodulo', name: 'Meu Subm√≥dulo', tables: ['TAB_NOVA_TABELA'] },
```

**Passo 3: Template ERP (banco de dados)**
```bash
# Atualizar template Intersolid
node update-template.js producao
```

**Passo 4: Commit e deploy**
```bash
git add -A
git commit -m "feat: Adiciona suporte a TAB_NOVA_TABELA"
git push origin TESTE

# Deploy
ssh root@46.202.150.64 "cd /root/prevencao-radar-repo && git pull origin TESTE && cd /root/clientes/tradicao && docker compose build --no-cache frontend backend && docker compose up -d --no-deps frontend backend && docker builder prune -f && docker image prune -f"
```

### ‚úÖ PROCESSO COMPLETO DE DEPLOY (COM VERIFICA√á√ÉO DE MAPEAMENTO)

```bash
# 1. ANTES DO COMMIT: Verificar se n√£o h√° hardcode
grep -r "INTERSOLID\." packages/backend/src --include="*.ts"
# Esperado: 0 matches (exceto coment√°rios)

# 2. Fazer commit e push
git add -A
git commit -m "feat: Nova funcionalidade com MappingService"
git push origin TESTE

# 3. Conectar na VPS
ssh root@46.202.150.64

# 4. Atualizar template ERP (se adicionou novas tabelas)
cd /root/prevencao-radar-repo
git pull origin TESTE
cd packages/backend
node update-template.js producao

# 5. Deploy normal
cd /root/clientes/tradicao
docker compose build --no-cache frontend backend
docker compose up -d --no-deps frontend backend

# 6. Limpar cache
docker builder prune -f && docker image prune -f

# 7. Verificar logs
docker logs prevencao-tradicao-backend --tail 30
```

### üéì Li√ß√£o Aprendida (05/02/2026)

**Problema:** Sistema foi deployado com c√≥digo usando MappingService, mas template ERP n√£o tinha a nova tabela configurada.

**Causa:** O c√≥digo usava `MappingService.getRealTableName('TAB_NF_ITEM', 'TAB_NF_ITEM')`, mas o template Intersolid no banco n√£o tinha `TAB_NF_ITEM` definido.

**Resultado:** O sistema usava o fallback (segundo par√¢metro), que funcionava para Intersolid mas n√£o seria configur√°vel para outros ERPs.

**Solu√ß√£o:**
1. Migra√ß√£o de ~479 refer√™ncias hardcoded para MappingService
2. Atualiza√ß√£o do template Intersolid com todas as tabelas necess√°rias
3. Atualiza√ß√£o do frontend para exibir as novas tabelas na configura√ß√£o

**Preven√ß√£o futura:**
- Sempre verificar se h√° hardcode ANTES do commit
- Ao adicionar nova tabela, atualizar: c√≥digo + frontend + template ERP
- Usar o script `update-template.js` para manter templates sincronizados

---

**√öltima atualiza√ß√£o:** 05/02/2026 - Adicionado regras de mapeamento din√¢mico e atualiza√ß√£o de templates ERP
**Criado por:** Claude (aprendendo com cada erro üéì)
